"""
è±†åŒ…(Doubao)é¡µé¢çˆ¬è™«
ä½¿ç”¨Playwrightçˆ¬å–è±†åŒ…èŠå¤©é¡µé¢ï¼Œæå–å¹¶è®°å½•ï¼š
1. æé—®çš„å…³é”®è¯
2. äº§å“åç§°å’Œæ’å
3. äº§å“æ¥æº
"""

import asyncio
import json
import re
import os
import sys
from datetime import datetime
from dataclasses import dataclass, field, asdict
from typing import Optional
from playwright.async_api import async_playwright, Page, Browser

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥llmæ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from llm.config_loader import create_random_llm_wrapper


@dataclass
class ProductInfo:
    """äº§å“ä¿¡æ¯"""
    rank: int
    name: str
    sources: list = field(default_factory=list)  # å¤šä¸ªæ¥æºåˆ—è¡¨ [{"title": "", "url": "", "source": ""}]


@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœ"""
    keyword: str
    crawl_time: str
    products: list[ProductInfo] = field(default_factory=list)
    raw_content: str = ""
    references: list[dict] = field(default_factory=list)


class DoubaoCrawler:
    """è±†åŒ…é¡µé¢çˆ¬è™«"""
    
    CN_NUM_MAP = {
        'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
        'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
    }
    
    def __init__(self, headless: bool = False, cookies: str = "", use_llm: bool = False):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            cookies: Cookieå­—ç¬¦ä¸²ï¼ˆä»æµè§ˆå™¨å¤åˆ¶ï¼‰
            use_llm: æ˜¯å¦åœ¨è§£æé˜¶æ®µè°ƒç”¨LLM
        """
        self.headless = headless
        self.cookies = cookies
        self.use_llm = use_llm
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.context = None
        
    def _parse_cookies(self, cookie_string: str, domain: str = ".doubao.com") -> list[dict]:
        """å°†cookieå­—ç¬¦ä¸²è§£æä¸ºPlaywrightæ ¼å¼çš„cookieåˆ—è¡¨"""
        cookies = []
        if not cookie_string:
            return cookies
            
        for item in cookie_string.split(';'):
            item = item.strip()
            if not item or '=' not in item:
                continue
            name, value = item.split('=', 1)
            cookies.append({
                "name": name.strip(),
                "value": value.strip(),
                "domain": domain,
                "path": "/",
            })
        return cookies
        
    async def start(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(headless=self.headless)
        self.context = await self.browser.new_context(
            viewport={"width": 1920, "height": 1080},
            locale="zh-CN"
        )
        
        # æ³¨å…¥cookie
        if self.cookies:
            parsed_cookies = self._parse_cookies(self.cookies)
            if parsed_cookies:
                await self.context.add_cookies(parsed_cookies)
                print(f"âœ“ å·²æ³¨å…¥ {len(parsed_cookies)} ä¸ªcookie")
        
        self.page = await self.context.new_page()
        
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            await self.browser.close()
            
    async def navigate_to_chat(self, url: str):
        """
        å¯¼èˆªåˆ°è±†åŒ…èŠå¤©é¡µé¢
        
        Args:
            url: è±†åŒ…èŠå¤©é¡µé¢URL
        """
        try:
            # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’ï¼Œä½¿ç”¨domcontentloadedåŠ å¿«åŠ è½½
            await self.page.goto(url, wait_until="domcontentloaded", timeout=60000)
        except Exception as e:
            print(f"âš  é¦–æ¬¡åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç­‰å¾…é¡µé¢ç¨³å®š: {e}")
            # å¦‚æœè¶…æ—¶ï¼Œå°è¯•ç­‰å¾…é¡µé¢ç¨³å®š
            await asyncio.sleep(5)
        
        # ç­‰å¾…Reactå†…å®¹æ¸²æŸ“å®Œæˆ
        print("â³ ç­‰å¾…é¡µé¢å†…å®¹æ¸²æŸ“...")
        await asyncio.sleep(8)
        
        # å°è¯•ç‚¹å‡»"å‚è€ƒèµ„æ–™"å±•å¼€å‚è€ƒé¢æ¿
        try:
            # æŸ¥æ‰¾åŒ…å«"å‚è€ƒ"æ–‡å­—çš„å…ƒç´ 
            ref_button = await self.page.query_selector('text=å‚è€ƒ')
            if ref_button:
                print("ğŸ“š æ‰¾åˆ°å‚è€ƒèµ„æ–™æŒ‰é’®ï¼Œç‚¹å‡»å±•å¼€...")
                await ref_button.click()
                # ç­‰å¾…å‚è€ƒé¢æ¿æ¸²æŸ“
                await asyncio.sleep(5)
            else:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                ref_selectors = [
                    '[class*="reference"]',
                    '[class*="source"]', 
                    'button:has-text("å‚è€ƒ")',
                    'span:has-text("ç¯‡èµ„æ–™")',
                ]
                for sel in ref_selectors:
                    try:
                        elem = await self.page.query_selector(sel)
                        if elem:
                            print(f"ğŸ“š æ‰¾åˆ°å‚è€ƒå…ƒç´  '{sel}'ï¼Œç‚¹å‡»å±•å¼€...")
                            await elem.click()
                            await asyncio.sleep(5)
                            break
                    except:
                        continue
        except Exception as e:
            print(f"âš  æ— æ³•å±•å¼€å‚è€ƒèµ„æ–™: {e}")
        
    async def send_question(self, question: str):
        """
        åœ¨è±†åŒ…ä¸­å‘é€é—®é¢˜
        
        Args:
            question: è¦å‘é€çš„é—®é¢˜
        """
        # æŸ¥æ‰¾è¾“å…¥æ¡†å¹¶è¾“å…¥é—®é¢˜
        input_selector = 'textarea[placeholder*="è¾“å…¥"], textarea[class*="input"], div[contenteditable="true"]'
        await self.page.wait_for_selector(input_selector, timeout=10000)
        input_element = await self.page.query_selector(input_selector)
        
        if input_element:
            await input_element.fill(question)
            await asyncio.sleep(0.5)
            
            # ç‚¹å‡»å‘é€æŒ‰é’®æˆ–æŒ‰å›è½¦
            send_btn = await self.page.query_selector('button[type="submit"], button[class*="send"]')
            if send_btn:
                await send_btn.click()
            else:
                await input_element.press("Enter")
                
            # ç­‰å¾…å›ç­”ç”Ÿæˆå®Œæˆ
            await self._wait_for_response()
            
    async def _wait_for_response(self, timeout: int = 60):
        """ç­‰å¾…è±†åŒ…å›ç­”ç”Ÿæˆå®Œæˆ"""
        # ç­‰å¾…åŠ è½½æŒ‡ç¤ºå™¨æ¶ˆå¤±
        await asyncio.sleep(2)
        
        for _ in range(timeout):
            # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨ç”Ÿæˆä¸­
            loading = await self.page.query_selector('[class*="loading"], [class*="typing"]')
            if not loading:
                break
            await asyncio.sleep(1)
            
        # é¢å¤–ç­‰å¾…ç¡®ä¿å†…å®¹å®Œå…¨åŠ è½½
        await asyncio.sleep(2)
        
    async def extract_content(self) -> dict:
        """
        æå–é¡µé¢å†…å®¹
        
        Returns:
            åŒ…å«ä¸»è¦å†…å®¹å’Œå‚è€ƒèµ„æ–™çš„å­—å…¸
        """
        result = {
            "main_content": "",
            "references": []
        }
        
        # ä¿å­˜é¡µé¢æˆªå›¾ç”¨äºè°ƒè¯•
        output_dir = os.path.dirname(os.path.abspath(__file__))
        screenshot_path = os.path.join(output_dir, "debug_screenshot.png")
        try:
            await self.page.screenshot(path=screenshot_path, full_page=True)
            print(f"ğŸ“· å·²ä¿å­˜é¡µé¢æˆªå›¾: {screenshot_path}")
        except Exception as e:
            print(f"âš  æˆªå›¾ä¿å­˜å¤±è´¥: {e}")
        
        # ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•
        html_path = os.path.join(output_dir, "debug_page.html")
        try:
            html_content = await self.page.content()
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(html_content)
            print(f"ğŸ“„ å·²ä¿å­˜é¡µé¢HTML: {html_path}")
        except Exception as e:
            print(f"âš  HTMLä¿å­˜å¤±è´¥: {e}")
        
        # è±†åŒ…é¡µé¢ç‰¹å®šçš„å†…å®¹é€‰æ‹©å™¨ï¼ˆæ›´æ–°ç‰ˆï¼‰
        content_selectors = [
            # è±†åŒ…èŠå¤©æ¶ˆæ¯
            '[class*="chat-message"]',
            '[class*="message-item"]',
            '[class*="message-content"]',
            '[class*="bot-message"]',
            '[class*="assistant"]',
            # Markdownå†…å®¹
            '[class*="markdown"]',
            '[class*="prose"]',
            # é€šç”¨å›å¤åŒºåŸŸ
            '[class*="answer"]',
            '[class*="response"]',
            '[class*="reply"]',
            '[class*="content"]',
            # æ–‡ç« /æ­£æ–‡
            'article',
            'main',
            '[role="main"]',
        ]
        
        # ä½¿ç”¨JavaScriptç›´æ¥è·å–é¡µé¢å†…å®¹ï¼ˆæ›´å¯é çš„æ–¹æ³•ï¼‰
        print("\nğŸ” æ­£åœ¨æå–é¡µé¢å†…å®¹...")
        
        try:
            # ä½¿ç”¨JavaScriptè·å–èŠå¤©å†…å®¹åŒºåŸŸçš„æ–‡æœ¬
            main_content = await self.page.evaluate('''() => {
                let content = '';
                
                // æ–¹æ³•1: æŸ¥æ‰¾h3æ ‡ç­¾ï¼ˆäº§å“æ ‡é¢˜é€šå¸¸åœ¨h3ä¸­ï¼‰
                const headings = document.querySelectorAll('h3, h2, h4');
                let foundProducts = [];
                for (const h of headings) {
                    const text = h.innerText.trim();
                    // æ£€æŸ¥æ˜¯å¦æ˜¯äº§å“æ ‡é¢˜ï¼ˆåŒ…å«ä¸­æ–‡åºå·ï¼‰
                    if (text && /^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€/.test(text)) {
                        foundProducts.push(text);
                        content += text + '\\n';
                    }
                }
                
                // æ–¹æ³•2: æŸ¥æ‰¾ul/liå…ƒç´ è·å–è¯¦ç»†ä¿¡æ¯
                const listItems = document.querySelectorAll('li');
                for (const li of listItems) {
                    const text = li.innerText.trim();
                    if (text && (
                        text.includes('æ ¸å¿ƒä¼˜åŠ¿') || 
                        text.includes('ç‰¹è‰²æœåŠ¡') ||
                        text.includes('é€‚åˆäººç¾¤')
                    )) {
                        content += text + '\\n';
                    }
                }
                
                // æ–¹æ³•3: å¦‚æœä¸Šé¢æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä»visibleæ–‡æœ¬ä¸­æå–
                if (!content || foundProducts.length === 0) {
                    // æŸ¥æ‰¾ä¸»è¦èŠå¤©åŒºåŸŸ
                    const chatAreas = document.querySelectorAll('[class*="message"], [class*="chat"], [class*="content"]');
                    for (const area of chatAreas) {
                        const text = area.innerText;
                        if (text && text.length > 200 && (
                            text.includes('æ’å¤©') || 
                            text.includes('æ ¸å¿ƒä¼˜åŠ¿')
                        )) {
                            content = text;
                            break;
                        }
                    }
                }
                
                // æ–¹æ³•4: æœ€åæ‰‹æ®µ - è·å–æ•´ä¸ªbodyæ–‡æœ¬
                if (!content || content.length < 100) {
                    // æ‰¾åˆ°æœ€å¤§çš„å†…å®¹åŒºåŸŸ
                    const main = document.querySelector('main') || document.body;
                    content = main.innerText;
                }
                
                return content;
            }''')
            
            if main_content:
                result["main_content"] = main_content
                print(f"ğŸ“ é€šè¿‡JavaScriptè·å–åˆ°å†…å®¹: {len(result['main_content'])} å­—ç¬¦")
        except Exception as e:
            print(f"âš  JavaScriptæå–å¤±è´¥: {e}")
        
        print(f"\nğŸ“ æå–åˆ°çš„å†…å®¹é•¿åº¦: {len(result['main_content'])} å­—ç¬¦")
        
        # ä½¿ç”¨JavaScriptæå–çœŸæ­£çš„å‚è€ƒèµ„æ–™ï¼ˆå¤–éƒ¨é“¾æ¥ï¼‰
        print("\nğŸ” æ­£åœ¨æå–å‚è€ƒèµ„æ–™...")
        references = await self._collect_panel_references(max_pages=5)
        result["references"] = references
        print(f"ğŸ“š æå–åˆ°çš„å‚è€ƒèµ„æ–™æ•°: {len(result['references'])}")
                        
        return result
    
    async def extract_rank_table(self) -> list[ProductInfo]:
        """
        ç›´æ¥è§£æé¡µé¢ä¸Šçš„è¡¨æ ¼ï¼Œæå–ã€æ’å-äº§å“/å¹³å°-å¼•ç”¨æ¥æºã€‘ç»“æ„
        """
        if not self.page:
            return []
        
        print("\nğŸ” å°è¯•ä»é¡µé¢è¡¨æ ¼ç›´æ¥æå–æ•°æ®...")
        try:
            table_rows = await self.page.evaluate("""() => {
                const normalize = (text) => (text || '').replace(/\\s+/g, '').toLowerCase();
                const tables = Array.from(document.querySelectorAll('table'));
                
                for (const table of tables) {
                    const headerRow = table.querySelector('thead tr') || table.querySelector('tr');
                    if (!headerRow) continue;
                    const headerCells = Array.from(headerRow.querySelectorAll('th, td'));
                    const map = {rank: -1, name: -1, source: -1};
                    
                    headerCells.forEach((cell, index) => {
                        const text = cell.innerText || '';
                        const n = normalize(text);
                        if (map.rank === -1 && (n.includes('æ’å') || n.includes('åºå·'))) {
                            map.rank = index;
                        }
                        if (map.name === -1 && (n.includes('äº§å“') || n.includes('å¹³å°') || n.includes('åº—') || n.includes('æœºæ„'))) {
                            map.name = index;
                        }
                        if (map.source === -1 && (n.includes('å¼•ç”¨') || n.includes('æ¥æº') || n.includes('å‚è€ƒ'))) {
                            map.source = index;
                        }
                    });
                    
                    if (map.rank === -1 || map.name === -1 || map.source === -1) {
                        continue;
                    }
                    
                    const bodyRows = table.querySelectorAll('tbody tr');
                    const dataRows = bodyRows.length ? Array.from(bodyRows) : Array.from(table.querySelectorAll('tr')).slice(1);
                    const rows = [];
                    
                    for (const row of dataRows) {
                        const cells = row.querySelectorAll('td');
                        if (!cells.length) continue;
                        
                        const rankCell = cells[map.rank] || cells[0];
                        const nameCell = cells[map.name] || cells[Math.min(map.name, cells.length - 1)];
                        const sourceCell = cells[map.source] || cells[Math.min(map.source, cells.length - 1)];
                        const nameText = nameCell ? nameCell.innerText.trim() : '';
                        
                        if (!nameText) {
                            continue;
                        }
                        
                        rows.push({
                            rankText: rankCell ? rankCell.innerText.trim() : '',
                            name: nameText,
                            sourceText: sourceCell ? sourceCell.innerText.trim() : '',
                            links: Array.from(sourceCell ? sourceCell.querySelectorAll('a[href]') : []).map(link => ({
                                title: (link.innerText || '').trim(),
                                url: link.href
                            }))
                        });
                    }
                    
                    if (rows.length >= 2) {
                        return rows;
                    }
                }
                
                return [];
            }""")
        except Exception as exc:
            print(f"âš  è¡¨æ ¼æå–å¤±è´¥: {exc}")
            return []
        
        if not table_rows:
            print("âš  è¡¨æ ¼ç»“æ„æœªæ£€æµ‹åˆ°ï¼Œç»§ç»­ä½¿ç”¨å…¶ä»–è§£æç­–ç•¥")
            return []
        
        products = []
        for idx, row in enumerate(table_rows, start=1):
            name = (row.get("name") or "").strip()
            if not name:
                continue
            
            rank = self._parse_rank_value(row.get("rankText"), fallback=idx)
            raw_source_text = (row.get("sourceText") or "").strip()
            products.append(ProductInfo(rank=rank, name=name, sources=[]))
            if raw_source_text:
                print(f"  âœ“ è¡¨æ ¼æ•°æ®: {rank}. {name} | æ¥æºæç¤º: {raw_source_text[:60]}")
            else:
                print(f"  âœ“ è¡¨æ ¼æ•°æ®: {rank}. {name}")
        
        products.sort(key=lambda p: p.rank)
        if products:
            print(f"ğŸ“Š ä»è¡¨æ ¼æå–åˆ° {len(products)} æ¡è®°å½•")
        else:
            print("âš  æœªæ‰¾åˆ°ç¬¦åˆè¦æ±‚çš„è¡¨æ ¼")
        return products
    
    async def _collect_panel_references(self, max_pages: int = 5) -> list[dict]:
        """æŠ“å–è±†åŒ…å‚è€ƒèµ„æ–™é¢æ¿çš„æ‰€æœ‰é“¾æ¥"""
        if not self.page:
            return []
        
        references: list[dict] = []
        seen_urls: set[str] = set()
        
        for page_idx in range(max_pages):
            page_refs = await self._extract_reference_links_once()
            new_count = 0
            for ref in page_refs:
                url = ref.get("url", "")
                title = ref.get("title", "").strip()
                summary = ref.get("summary", "").strip()
                
                if url and url in seen_urls:
                    continue
                if url:
                    seen_urls.add(url)
                
                ref["title"] = title
                ref["summary"] = summary
                ref["source"] = self._extract_source_name(url) if url else (ref.get("source_hint") or "æœªçŸ¥æ¥æº")
                references.append(ref)
                new_count += 1
            
            if new_count == 0:
                # æ²¡æœ‰æ–°å¢å†…å®¹ï¼Œåœæ­¢ç¿»é¡µ
                break
            
            has_next = await self._goto_next_reference_page()
            if not has_next:
                break
        
        print(f"ğŸ“š å‚è€ƒèµ„æ–™é¢æ¿å…±æå– {len(references)} æ¡")
        return references
    
    async def _extract_reference_links_once(self) -> list[dict]:
        """åœ¨å½“å‰å‚è€ƒèµ„æ–™é¢æ¿ä¸­æå–é“¾æ¥"""
        if not self.page:
            return []
        
        try:
            refs = await self.page.evaluate("""() => {
                const selectors = [
                    '[data-testid*=\"reference\"]',
                    '[class*=\"reference\"]',
                    '[class*=\"references\"]',
                    '[class*=\"citation\"]',
                    '[class*=\"source-list\"]'
                ];
                
                const containers = [];
                for (const sel of selectors) {
                    document.querySelectorAll(sel).forEach(elem => containers.push(elem));
                }
                
                if (!containers.length) {
                    const fallback = Array.from(document.querySelectorAll('section,div'))
                        .filter(elem => {
                            const text = (elem.innerText || '').trim();
                            return text.includes('å‚è€ƒ') && elem.querySelectorAll('a[href^=\"http\"]').length >= 1;
                        });
                    containers.push(...fallback);
                }
                
                const seen = new Set();
                const results = [];
                
                for (const container of containers) {
                    const cards = container.querySelectorAll('[data-testid*=\"reference\"], [class*=\"reference-item\"], li, article, [class*=\"item\"], [class*=\"card\"]');
                    for (const card of cards) {
                        const link = card.querySelector('a[href^=\"http\"]');
                        if (!link) continue;
                        
                        const href = link.href;
                        if (!href || href.includes('doubao.com') || href.includes('bytedance.com')) {
                            continue;
                        }
                        if (seen.has(href)) continue;
                        seen.add(href);
                        
                        const titleElem = card.querySelector('[class*=\"title\"], h3, h4, h5, strong') || link;
                        const summaryElem = card.querySelector('[class*=\"summary\"], [class*=\"desc\"], p');
                        const sourceElem = card.querySelector('[class*=\"site\"], [class*=\"source\"], span');
                        
                        const title = titleElem && titleElem.innerText ? titleElem.innerText.trim() : (link.innerText || '').trim();
                        const summary = summaryElem && summaryElem.innerText ? summaryElem.innerText.trim() : '';
                        const source = sourceElem && sourceElem.innerText ? sourceElem.innerText.trim() : '';
                        
                        results.push({
                            title,
                            url: href,
                            summary,
                            source_hint: source
                        });
                    }
                    
                    if (results.length >= 3) {
                        // å½“å‰å®¹å™¨å·²ç»æœ‰ç»“æœï¼Œé¿å…ç»§ç»­éå†å…¶å®ƒå®¹å™¨å¯¼è‡´é‡å¤
                        break;
                    }
                }
                
                if (!results.length) {
                    const allLinks = document.querySelectorAll('a[href^=\"http\"]');
                    for (const link of allLinks) {
                        const href = link.href;
                        if (!href || href.includes('doubao.com') || href.includes('bytedance.com')) continue;
                        const parentText = (link.closest('div,li,article')?.innerText || '').trim();
                        if (parentText.includes('å‚è€ƒ') || parentText.includes('å¼•ç”¨')) {
                            results.push({
                                title: (link.innerText || '').trim(),
                                url: href,
                                summary: parentText.substring(0, 120),
                                source_hint: ''
                            });
                        }
                    }
                }
                
                return results;
            }""")
            return refs or []
        except Exception as exc:
            print(f"âš  å‚è€ƒèµ„æ–™æå–å¤±è´¥: {exc}")
            return []
    
    async def _goto_next_reference_page(self) -> bool:
        """ç¿»é¡µæˆ–æ»šåŠ¨ä»¥åŠ è½½æ›´å¤šå‚è€ƒèµ„æ–™"""
        if not self.page:
            return False
        
        try:
            clicked = await self.page.evaluate("""() => {
                const clickSelectors = ['button', 'a', 'div'];
                for (const tag of clickSelectors) {
                    const candidates = Array.from(document.querySelectorAll(tag)).filter(elem => {
                        const text = (elem.innerText || '').trim();
                        return /ä¸‹ä¸€é¡µ|æŸ¥çœ‹æ›´å¤š|æ›´å¤šå‚è€ƒ|å±•å¼€æ›´å¤š/.test(text);
                    });
                    for (const btn of candidates) {
                        if (btn.disabled || btn.getAttribute('aria-disabled') === 'true') {
                            continue;
                        }
                        btn.click();
                        return true;
                    }
                }
                
                const panelSelectors = [
                    '[data-testid*=\"reference\"]',
                    '[class*=\"reference-list\"]',
                    '[class*=\"reference-panel\"]'
                ];
                for (const sel of panelSelectors) {
                    const panel = document.querySelector(sel);
                    if (panel && panel.scrollHeight - panel.clientHeight > 20) {
                        const before = panel.scrollTop;
                        panel.scrollTop = panel.scrollHeight;
                        return panel.scrollTop !== before;
                    }
                }
                return false;
            }""")
            if clicked:
                await asyncio.sleep(1.5)
            return clicked
        except Exception as exc:
            print(f"âš  å‚è€ƒèµ„æ–™ç¿»é¡µå¤±è´¥: {exc}")
            return False
    
    async def search_online_for_product(self, product_name: str, keyword: str = "") -> list:
        """
        å…¼å®¹æ—§æ¥å£ï¼šæ ¹æ®æœ€æ–°ç­–ç•¥ç¦ç”¨å¤–éƒ¨æœç´¢
        """
        print(f"âš  æœç´¢å·¥å…·å·²ç¦ç”¨ï¼Œè·³è¿‡å¯¹ '{product_name}' çš„åœ¨çº¿æœç´¢è¯·æ±‚")
        return []
    
    def _parse_rank_value(self, value, fallback: int) -> int:
        """å°†è¡¨æ ¼ä¸­çš„æ’åå­—æ®µè½¬æ¢ä¸ºæ•´æ•°"""
        if isinstance(value, (int, float)):
            ivalue = int(value)
            if ivalue > 0:
                return ivalue
        text = str(value or "").strip()
        if not text:
            return fallback
        digit_match = re.search(r'\d+', text)
        if digit_match:
            return int(digit_match.group())
        total = 0
        for char in text:
            total += self.CN_NUM_MAP.get(char, 0)
        return total or fallback
    
    def _extract_source_name(self, url: str) -> str:
        """ä»URLæå–æ¥æºåç§°"""
        if not url:
            return "æœªçŸ¥æ¥æº"
            
        # å¸¸è§ç½‘ç«™æ˜ å°„
        source_map = {
            "zhihu.com": "çŸ¥ä¹",
            "xiaohongshu.com": "å°çº¢ä¹¦",
            "baidu.com": "ç™¾åº¦",
            "sohu.com": "æœç‹",
            "sina.com": "æ–°æµª",
            "163.com": "ç½‘æ˜“",
            "qq.com": "è…¾è®¯",
            "weibo.com": "å¾®åš",
            "bilibili.com": "Bç«™",
            "douban.com": "è±†ç“£",
            "taobao.com": "æ·˜å®",
            "jd.com": "äº¬ä¸œ",
            "xnnews.com.cn": "å’¸å®ç½‘",
            "wandoujia.com": "è±Œè±†èš",
            "toutiao.com": "ä»Šæ—¥å¤´æ¡",
            "csdn.net": "CSDN",
        }
        
        for domain, name in source_map.items():
            if domain in url:
                return name
        
        domain_match = re.search(r'https?://([^/]+)/?', url)
        if domain_match:
            host = domain_match.group(1)
            return host.replace("www.", "")
        return "æœªçŸ¥æ¥æº"
    
    def _normalize_text(self, text: str) -> str:
        """ç»Ÿä¸€æ–‡æœ¬æ ¼å¼ç”¨äºåŒ¹é…"""
        if not text:
            return ""
        normalized = re.sub(r'[\sÂ·â€¢ï¼Œ,ã€‚ã€â€œâ€\"\'()ï¼ˆï¼‰ã€ã€‘\\[\\]ã€Šã€‹<>â€”-]', '', text)
        return normalized.lower()
    
    def _match_references_to_products(self, products: list[ProductInfo], references: list[dict]):
        """æ ¹æ®å‚è€ƒèµ„æ–™åŒ¹é…å¼•ç”¨æ¥æº"""
        if not products or not references:
            return
        
        normalized_refs = []
        for ref in references:
            title = ref.get("title", "")
            url = ref.get("url", "")
            summary_blob = " ".join(filter(None, [
                ref.get("summary", ""),
                ref.get("content", ""),
                ref.get("source_hint", ""),
            ]))
            normalized_refs.append({
                "raw": ref,
                "normalized_title": self._normalize_text(title),
                "normalized_content": self._normalize_text(summary_blob),
                "title": title or ref.get("source", "å‚è€ƒèµ„æ–™"),
                "url": url,
                "source": ref.get("source") or self._extract_source_name(url)
            })
        
        for product in products:
            normalized_name = self._normalize_text(product.name)
            if not normalized_name:
                continue
            
            if not hasattr(product, "sources") or not isinstance(product.sources, list):
                product.sources = []
            
            existing = set()
            for src in product.sources:
                key = src.get("url") or src.get("title")
                if key:
                    existing.add(key)
            
            new_added = 0
            for ref in normalized_refs:
                if not ref["normalized_title"] and not ref["normalized_content"]:
                    continue
                if normalized_name not in ref["normalized_title"] and normalized_name not in ref["normalized_content"]:
                    continue
                
                key = ref["url"] or ref["title"]
                if key in existing:
                    continue
                existing.add(key)
                
                product.sources.append({
                    "title": ref["title"],
                    "url": ref["url"],
                    "source": ref["source"]
                })
                new_added += 1
            
            if new_added:
                print(f"    âœ“ å¼•ç”¨åŒ¹é…: {product.name} (æ–°å¢{new_added}æ¡)")
            
    async def fetch_reference_contents(self, references: list, max_refs: int = 5) -> list:
        """
        è·å–å‚è€ƒèµ„æ–™ç½‘é¡µçš„æ­£æ–‡å†…å®¹
        
        Args:
            references: å‚è€ƒèµ„æ–™åˆ—è¡¨
            max_refs: æœ€å¤šè·å–å¤šå°‘ä¸ªå‚è€ƒèµ„æ–™çš„å†…å®¹
            
        Returns:
            æ›´æ–°åçš„å‚è€ƒèµ„æ–™åˆ—è¡¨ï¼ŒåŒ…å«æ­£æ–‡å†…å®¹
        """
        print(f"\nğŸ“– æ­£åœ¨è·å–å‚è€ƒèµ„æ–™ç½‘é¡µå†…å®¹ (æœ€å¤š{max_refs}ä¸ª)...")
        
        for i, ref in enumerate(references[:max_refs]):
            url = ref.get("url", "")
            if not url:
                continue
                
            try:
                print(f"  [{i+1}/{min(len(references), max_refs)}] è®¿é—®: {url[:50]}...")
                
                # åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€å‚è€ƒé“¾æ¥
                page = await self.context.new_page()
                await page.goto(url, wait_until="domcontentloaded", timeout=15000)
                await asyncio.sleep(2)  # ç­‰å¾…å†…å®¹æ¸²æŸ“
                
                # æå–é¡µé¢æ­£æ–‡
                content = await page.evaluate('''() => {
                    // ç§»é™¤è„šæœ¬ã€æ ·å¼ç­‰æ— ç”¨å…ƒç´ 
                    const removeElements = document.querySelectorAll('script, style, noscript, iframe, nav, header, footer, aside');
                    
                    // å°è¯•è·å–ä¸»è¦å†…å®¹åŒºåŸŸ
                    const mainSelectors = [
                        'article',
                        '[class*="content"]',
                        '[class*="article"]',
                        '[class*="post"]',
                        'main',
                        '.main',
                        '#content',
                        '#main'
                    ];
                    
                    let content = '';
                    for (const sel of mainSelectors) {
                        const elem = document.querySelector(sel);
                        if (elem && elem.innerText.length > 200) {
                            content = elem.innerText;
                            break;
                        }
                    }
                    
                    // å¦‚æœæ²¡æ‰¾åˆ°ä¸»è¦å†…å®¹ï¼Œä½¿ç”¨body
                    if (!content || content.length < 200) {
                        content = document.body.innerText;
                    }
                    
                    // æ¸…ç†ç©ºç™½å­—ç¬¦
                    return content.replace(/\\s+/g, ' ').substring(0, 3000);
                }''')
                
                await page.close()
                
                if content and len(content) > 100:
                    ref["content"] = content
                    print(f"    âœ“ è·å–åˆ° {len(content)} å­—ç¬¦")
                else:
                    ref["content"] = ""
                    print(f"    âš  å†…å®¹å¤ªå°‘æˆ–è·å–å¤±è´¥")
                    
            except Exception as e:
                print(f"    âš  è·å–å¤±è´¥: {e}")
                ref["content"] = ""
        
        return references

    async def parse_products_with_llm(self, content: str, references: list) -> list[ProductInfo]:
        """
        ä½¿ç”¨LLMæ™ºèƒ½è§£æå†…å®¹ä¸­çš„äº§å“ä¿¡æ¯
        
        Args:
            content: ä¸»è¦å†…å®¹æ–‡æœ¬
            references: å‚è€ƒèµ„æ–™åˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å«contentå­—æ®µï¼‰
            
        Returns:
            äº§å“ä¿¡æ¯åˆ—è¡¨
        """
        print("\nğŸ¤– æ­£åœ¨ä½¿ç”¨LLMè§£æå†…å®¹...")
        
        # è·å–é…ç½®æ–‡ä»¶è·¯å¾„
        config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config.json")
        
        # åˆ›å»ºLLM wrapper
        llm = create_random_llm_wrapper(config_path)
        if not llm:
            print("âš  æ— æ³•åˆ›å»ºLLM wrapperï¼Œå°†ä½¿ç”¨æ­£åˆ™è§£æ")
            return await self._parse_products_regex(content, references)
        
        # å‡†å¤‡å¼•ç”¨æ¥æºä¿¡æ¯ï¼ŒåŒ…å«æ­£æ–‡å†…å®¹
        refs_text = ""
        if references:
            refs_parts = []
            for i, ref in enumerate(references[:10], 1):
                title = ref.get('title', 'æœªçŸ¥')
                url = ref.get('url', '')
                ref_content = ref.get('content', '')
                
                if ref_content:
                    # åŒ…å«æ­£æ–‡å†…å®¹çš„å‚è€ƒèµ„æ–™
                    refs_parts.append(f"""å‚è€ƒèµ„æ–™{i}:
æ ‡é¢˜: {title}
URL: {url}
æ­£æ–‡æ‘˜è¦: {ref_content[:1500]}
---""")
                else:
                    refs_parts.append(f"å‚è€ƒèµ„æ–™{i}: {title} ({url})")
            
            refs_text = "\n".join(refs_parts)
        
        # æ„å»ºæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è§£æåŠ©æ‰‹ã€‚ä½ éœ€è¦ä»è±†åŒ…AIçš„å›ç­”ä¸­æå–äº§å“/å•†å®¶çš„æ’ååˆ—è¡¨ï¼Œå¹¶ä»å‚è€ƒèµ„æ–™ä¸­åŒ¹é…æ‰€æœ‰æåˆ°è¯¥å•†å®¶çš„æ¥æºã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
{
    "products": [
        {
            "rank": 1,
            "name": "äº§å“/å•†å®¶åç§°",
            "features": "æ ¸å¿ƒç‰¹ç‚¹å’Œä¼˜åŠ¿çš„ç®€è¦æè¿°",
            "sources": [
                {"title": "æ¥æºæ ‡é¢˜1", "url": "æ¥æºURL1"},
                {"title": "æ¥æºæ ‡é¢˜2", "url": "æ¥æºURL2"}
            ]
        }
    ]
}

åŒ¹é…è§„åˆ™ï¼š
1. rankæ˜¯æ’åé¡ºåºï¼Œä»1å¼€å§‹
2. nameæ˜¯äº§å“æˆ–å•†å®¶çš„åç§°ï¼Œå»æ‰emojiï¼ˆğŸ”°ğŸ’ç­‰ï¼‰ã€åºå·ï¼ˆä¸€ã€äºŒã€1.ç­‰ï¼‰ç­‰å‰ç¼€
3. featuresæ˜¯äº§å“çš„æ ¸å¿ƒç‰¹ç‚¹æè¿°ï¼Œç®€æ´æ˜äº†
4. é‡è¦ï¼šsourcesæ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰æåˆ°è¯¥å•†å®¶çš„å‚è€ƒèµ„æ–™
5. ä»”ç»†é˜…è¯»æ¯ä¸ªå‚è€ƒèµ„æ–™çš„æ ‡é¢˜å’Œæ­£æ–‡æ‘˜è¦ï¼Œåªè¦æ­£æ–‡ä¸­æåˆ°äº†è¯¥å•†å®¶/äº§å“åç§°ï¼Œå°±æ·»åŠ åˆ°sourcesä¸­
6. ä¸€ä¸ªå•†å®¶å¯èƒ½è¢«å¤šä¸ªå‚è€ƒèµ„æ–™æåˆ°ï¼Œå…¨éƒ¨æ·»åŠ åˆ°sourcesæ•°ç»„
7. å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ¥æºï¼Œsourcesä¸ºç©ºæ•°ç»„[]
8. åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–ä»»ä½•æ–‡å­—"""

        user_prompt = f"""è¯·ä»ä»¥ä¸‹è±†åŒ…AIçš„å›ç­”ä¸­æå–äº§å“/å•†å®¶æ’ååˆ—è¡¨ï¼š

=== å›ç­”å†…å®¹ ===
{content[:6000]}

=== å‚è€ƒèµ„æ–™è¯¦æƒ… ===
{refs_text if refs_text else "æ— å‚è€ƒèµ„æ–™"}

ä»»åŠ¡ï¼š
1. æå–æ‰€æœ‰æåˆ°çš„äº§å“/å•†å®¶åç§°å’Œæ’å
2. ä»”ç»†é˜…è¯»æ¯ä¸ªå‚è€ƒèµ„æ–™çš„æ ‡é¢˜å’Œæ­£æ–‡ï¼ŒæŸ¥æ‰¾æ˜¯å¦åŒ…å«è¿™äº›å•†å®¶åç§°
3. å¦‚æœå¤šä¸ªå‚è€ƒèµ„æ–™çš„æ­£æ–‡ä¸­éƒ½æåˆ°äº†æŸä¸ªå•†å®¶ï¼Œå°†æ‰€æœ‰è¿™äº›å‚è€ƒèµ„æ–™éƒ½æ·»åŠ åˆ°è¯¥å•†å®¶çš„sourcesæ•°ç»„ä¸­
4. è¿”å›JSONæ ¼å¼çš„ç»“æœ"""

        try:
            response = await llm.call(user_prompt, system_prompt)
            await llm.close()
            
            # è§£æJSONå“åº”
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                json_str = json_match.group(0)
                result = json.loads(json_str)
                
                products = []
                for item in result.get("products", []):
                    product = ProductInfo(
                        rank=item.get("rank", 0),
                        name=item.get("name", ""),
                        sources=[]
                    )
                    products.append(product)
                    print(f"  âœ“ LLMè§£æ: {item.get('rank')}. {item.get('name')}")
                
                self._match_references_to_products(products, references)
                
                print(f"ğŸ“¦ LLMå…±è§£æåˆ° {len(products)} ä¸ªäº§å“ï¼Œå¹¶å®Œæˆå‚è€ƒèµ„æ–™åŒ¹é…")
                return products
            else:
                print(f"âš  LLMå“åº”æ ¼å¼é”™è¯¯: {response[:200]}")
                return await self._parse_products_regex(content, references)
                
        except Exception as e:
            print(f"âš  LLMè§£æå¤±è´¥: {e}")
            await llm.close()
            return await self._parse_products_regex(content, references)
    
    async def _parse_products_regex(self, content: str, references: list) -> list[ProductInfo]:
        """
        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æäº§å“ä¿¡æ¯ï¼ˆå¤‡é€‰æ–¹æ³•ï¼‰
        """
        products = []
        
        print(f"\nğŸ” æ­£åœ¨ä½¿ç”¨æ­£åˆ™è§£æå†…å®¹... (é•¿åº¦: {len(content)})")
        
        # è§£æäº§å“åç§°å’Œæ’å
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ¨¡å¼1ï¼šä¸­æ–‡åºå·æ ¼å¼ "ä¸€ã€æ’å¤©å¥¢ä¾ˆå“"
            match = re.match(r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)ã€\s*(.+?)(?:ï¼ˆ|ã€|$)', line)
            if match:
                cn_rank = match.group(1)
                name = match.group(2).strip()
                rank = sum(self.CN_NUM_MAP.get(char, 0) for char in cn_rank)
                
                if name and len(name) < 100:
                    products.append(ProductInfo(rank=rank, name=name, sources=[]))
                    print(f"  âœ“ æ­£åˆ™æ‰¾åˆ°: {rank}. {name}")
                continue
            
            # æ¨¡å¼2ï¼šæ•°å­—åºå·æ ¼å¼ "1. äº§å“å"
            match = re.match(r'^(\d+)[\.ã€ï¼‰\)]\s*(.+)', line)
            if match:
                rank = int(match.group(1))
                name = re.sub(r'\[citation:\d+\]|ã€\d+ã€‘', '', match.group(2)).strip()
                
                if name and len(name) < 100:
                    products.append(ProductInfo(rank=rank, name=name, sources=[]))
                    print(f"  âœ“ æ­£åˆ™æ‰¾åˆ°: {rank}. {name}")
                continue
            
            # æ¨¡å¼3ï¼šemojiå‰ç¼€æ ¼å¼ "ğŸ”° æ’å¤©å¥¢ä¾ˆå“"
            match = re.match(r'^[^\w\u4e00-\u9fff]*\s*(.+?)(?:ï¼ˆ|\(|$)', line)
            if match:
                name = match.group(1).strip()
                # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„åç§°
                if name and 2 < len(name) < 50 and any(c in name for c in ['åº—', 'å“', 'å®', 'è¡Œ', 'å®¶', 'é¦†']):
                    rank = len(products) + 1
                    products.append(ProductInfo(rank=rank, name=name, sources=[]))
                    print(f"  âœ“ æ­£åˆ™æ‰¾åˆ°: {rank}. {name}")
        
        self._match_references_to_products(products, references)
        print(f"ğŸ“¦ æ­£åˆ™å…±è§£æåˆ° {len(products)} ä¸ªäº§å“")
        return products
    
    async def parse_products(self, content: str, references: list, use_llm: Optional[bool] = None) -> list[ProductInfo]:
        """
        è§£æå†…å®¹ä¸­çš„äº§å“ä¿¡æ¯
        
        Args:
            content: ä¸»è¦å†…å®¹æ–‡æœ¬
            references: å‚è€ƒèµ„æ–™åˆ—è¡¨
            use_llm: æ˜¯å¦ä½¿ç”¨LLMè§£æï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            äº§å“ä¿¡æ¯åˆ—è¡¨
        """
        use_llm = self.use_llm if use_llm is None else use_llm
        
        if use_llm and content:
            return await self.parse_products_with_llm(content, references)
        else:
            return await self._parse_products_regex(content, references)
    
    async def crawl(self, url: str, keyword: str = "") -> CrawlResult:
        """
        çˆ¬å–è±†åŒ…é¡µé¢
        
        Args:
            url: è±†åŒ…èŠå¤©é¡µé¢URL
            keyword: å…³é”®è¯ï¼ˆå¦‚æœä¸ºç©ºï¼Œå°†å°è¯•ä»é¡µé¢æå–ï¼‰
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        await self.navigate_to_chat(url)
        
        table_products = await self.extract_rank_table()
        
        # æå–å†…å®¹
        content_data = await self.extract_content()
        
        # å¦‚æœæ²¡æœ‰æä¾›å…³é”®è¯ï¼Œå°è¯•ä»URLæˆ–é¡µé¢æå–
        if not keyword:
            # å°è¯•ä»é¡µé¢æ ‡é¢˜æˆ–è¾“å…¥æ¡†æå–
            title = await self.page.title()
            keyword = title.replace("è±†åŒ…", "").replace("-", "").strip()
        
        # 1. ç¬¬ä¸€è½®è§£æï¼šä½¿ç”¨ç°æœ‰å‚è€ƒèµ„æ–™ï¼ˆä»…æ ‡é¢˜/URLï¼‰
        references = content_data["references"]
        
        fetched_refs = False
        
        if table_products:
            products = table_products
            print("âœ… å·²é€šè¿‡è¡¨æ ¼å®Œæˆç»“æ„åŒ–æå–ï¼Œè·³è¿‡LLMè§£æ")
            self._match_references_to_products(products, references)
        else:
            products = await self.parse_products(
                content_data["main_content"], 
                references,
                use_llm=self.use_llm
            )
            
        products_without_source = [p for p in products if not p.sources]
        if products_without_source and references:
            print(f"\nâš¡ å‘ç° {len(products_without_source)} ä¸ªäº§å“ç¼ºå°‘æ¥æºï¼Œå°è¯•ä½¿ç”¨å‚è€ƒèµ„æ–™æ­£æ–‡åŒ¹é…...")
            references = await self.fetch_reference_contents(references, max_refs=5)
            fetched_refs = True
            self._match_references_to_products(products, references)
            products_without_source = [p for p in products if not p.sources]
            if products_without_source:
                for p in products_without_source:
                    print(f"  âš  ä»æœªä¸º {p.name} æ‰¾åˆ°å¼•ç”¨ï¼Œä¿ç•™ä¸ºç©º")
        
        if fetched_refs:
            content_data["references"] = references
        
        return CrawlResult(
            keyword=keyword,
            crawl_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            products=products,
            raw_content=content_data["main_content"],
            references=content_data["references"]
        )


def generate_html_report(results: list[CrawlResult], output_path: str):
    """
    ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š
    
    Args:
        results: çˆ¬å–ç»“æœåˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    html_template = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è±†åŒ…çˆ¬å–ç»“æœæŠ¥å‘Š</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: 'Microsoft YaHei', 'PingFang SC', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        h1 {
            color: white;
            text-align: center;
            margin-bottom: 30px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }
        .result-card {
            background: white;
            border-radius: 16px;
            padding: 24px;
            margin-bottom: 24px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.15);
        }
        .keyword-header {
            background: linear-gradient(90deg, #4CAF50, #45a049);
            color: white;
            padding: 12px 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            font-size: 18px;
            font-weight: bold;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 16px;
        }
        th {
            background: linear-gradient(90deg, #4CAF50, #45a049);
            color: white;
            padding: 14px 16px;
            text-align: left;
            font-weight: 600;
        }
        th:first-child {
            border-radius: 8px 0 0 0;
            width: 80px;
        }
        th:last-child {
            border-radius: 0 8px 0 0;
        }
        td {
            padding: 14px 16px;
            border-bottom: 1px solid #eee;
        }
        tr:nth-child(even) {
            background-color: #f8f9fa;
        }
        tr:hover {
            background-color: #e8f5e9;
        }
        .rank-cell {
            font-weight: bold;
            color: #4CAF50;
            font-size: 18px;
        }
        .product-name {
            color: #1976D2;
            font-weight: 500;
        }
        .source-link {
            color: #666;
        }
        .source-link a {
            color: #1976D2;
            text-decoration: none;
        }
        .source-link a:hover {
            text-decoration: underline;
        }
        .meta-info {
            color: #888;
            font-size: 14px;
            margin-top: 16px;
            padding-top: 16px;
            border-top: 1px solid #eee;
        }
        .no-data {
            text-align: center;
            color: #999;
            padding: 40px;
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ” è±†åŒ…çˆ¬å–ç»“æœæŠ¥å‘Š</h1>
        {content}
    </div>
</body>
</html>
"""
    
    content_html = ""
    
    for result in results:
        card_html = f"""
        <div class="result-card">
            <div class="keyword-header">å…³é”®è¯ï¼š{result.keyword}</div>
            <table>
                <thead>
                    <tr>
                        <th>æ’å</th>
                        <th>äº§å“/å¹³å°</th>
                        <th>å¼•ç”¨æ¥æº</th>
                    </tr>
                </thead>
                <tbody>
"""
        
        if result.products:
            for product in result.products:
                # å¤„ç†å¤šä¸ªæ¥æº
                source_display = "-"
                if hasattr(product, 'sources') and product.sources:
                    links = []
                    for src in product.sources:
                        if isinstance(src, dict):
                            title = src.get('title', 'æœªçŸ¥æ¥æº')
                            url = src.get('url', '')
                            if url:
                                links.append(f'<div style="margin-bottom: 5px;"><a href="{url}" target="_blank">{title}</a></div>')
                            else:
                                links.append(f'<div>{title}</div>')
                    if links:
                        source_display = "".join(links)
                # å…¼å®¹æ—§å­—æ®µ
                elif hasattr(product, 'source') and product.source and product.source_url:
                     source_display = f'<a href="{product.source_url}" target="_blank">{product.source} ({product.source_url})</a>'
                elif hasattr(product, 'source') and product.source:
                     source_display = product.source
                    
                card_html += f"""
                    <tr>
                        <td class="rank-cell">{product.rank}</td>
                        <td class="product-name">{product.name}</td>
                        <td class="source-link">{source_display}</td>
                    </tr>
"""
        else:
            card_html += """
                    <tr>
                        <td colspan="3" class="no-data">æš‚æ— äº§å“æ•°æ®</td>
                    </tr>
"""
            
        card_html += f"""
                </tbody>
            </table>
            <div class="meta-info">
                çˆ¬å–æ—¶é—´ï¼š{result.crawl_time} | 
                å‚è€ƒèµ„æ–™æ•°é‡ï¼š{len(result.references)}
            </div>
        </div>
"""
        content_html += card_html
        
    html_content = html_template.replace("{content}", content_html)
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html_content)
        
    print(f"æŠ¥å‘Šå·²ç”Ÿæˆï¼š{output_path}")


def save_json_result(results: list[CrawlResult], output_path: str):
    """ä¿å­˜JSONæ ¼å¼çš„ç»“æœ"""
    data = []
    for result in results:
        data.append({
            "keyword": result.keyword,
            "crawl_time": result.crawl_time,
            "products": [asdict(p) for p in result.products],
            "references": result.references
        })
        
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
        
    print(f"JSONç»“æœå·²ä¿å­˜ï¼š{output_path}")


async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # ç¤ºä¾‹URLï¼ˆéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„è±†åŒ…èŠå¤©é¡µé¢URLï¼‰
    test_url = "https://doubao.com/chat/3841056122567454"
    keyword = "äºŒæ‰‹å¥¢ä¾ˆå“ä¸Šé—¨å›æ”¶"
    
    crawler = DoubaoCrawler(headless=False)
    
    try:
        await crawler.start()
        
        # çˆ¬å–é¡µé¢
        result = await crawler.crawl(test_url, keyword)
        
        # ä¿å­˜ç»“æœ
        results = [result]
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        output_dir = os.path.dirname(os.path.abspath(__file__))
        generate_html_report(results, os.path.join(output_dir, "report.html"))
        
        # ä¿å­˜JSONç»“æœ
        save_json_result(results, os.path.join(output_dir, "results.json"))
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print(f"\n{'='*50}")
        print(f"å…³é”®è¯: {result.keyword}")
        print(f"çˆ¬å–æ—¶é—´: {result.crawl_time}")
        print(f"äº§å“æ•°é‡: {len(result.products)}")
        print(f"å‚è€ƒèµ„æ–™æ•°é‡: {len(result.references)}")
        print(f"{'='*50}")
        
        for product in result.products:
            print(f"{product.rank}. {product.name} - {product.source}")
            
    finally:
        await crawler.close()


if __name__ == "__main__":
    asyncio.run(main())
